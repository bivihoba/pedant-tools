#!/usr/bin/python
import __init__,os,sys,uuid,glob,argparse,re,json
from pedant.crawler_app import Application

#parse arguments
parser = argparse.ArgumentParser()
parser.add_argument( "-u", "--url", type=str , help="Host for scanning. Example: http://host.name.com/")
parser.add_argument( "-w", "--workers", type=int , choices=xrange(1,21),help="Count of parallel workers", default=4)
parser.add_argument( "-t", "--timeout", type=int , choices=xrange(10,210,10),help="Timeout of waiting data from parser<=>grabber", default=60)
args = parser.parse_args()

app = Application( { 
		'timeout' : args.timeout,
		'base_url': args.url,
		'workers_cnt': args.workers,
		'blacklist':[ re.compile(".*&amp;amp.*") ] } )
app.start()